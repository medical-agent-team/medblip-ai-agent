# Copy this file to `.env` and fill in values as needed.

# ===== LLM Configuration =====
# Choose ONE of the following configurations:

# OPTION 1: Local vLLM (recommended for development/testing)
# Uncomment these lines if you're running vLLM locally on port 8000
# See vllm/instruction.md for setup instructions
# OPENAI_API_KEY=dummy
# OPENAI_API_BASE=http://localhost:8000/v1
# OPENAI_MODEL=Qwen3-0.6B

# OPTION 2: OpenAI API (requires valid API key)
# Uncomment these lines and add your OpenAI API key
# OPENAI_API_KEY=sk-your-openai-api-key-here
# OPENAI_API_BASE=https://api.openai.com/v1
# OPENAI_MODEL=gpt-4o-mini

# ===== Langfuse Observability =====
# Optional: Enable Langfuse tracing for LLM observability
# Set to "true" to enable, "false" to disable (default: false)
LANGFUSE_ENABLED=false

# Required if LANGFUSE_ENABLED=true: Langfuse API credentials
# Option 1: Get from Langfuse Cloud dashboard (https://cloud.langfuse.com)
# Option 2: For self-hosted, get from your Langfuse server UI (Settings â†’ API Keys)
#           or use headless init values from langfuse/.env
LANGFUSE_PUBLIC_KEY=
LANGFUSE_SECRET_KEY=

# Optional: Langfuse host URL
# Leave empty for Langfuse Cloud (default: https://cloud.langfuse.com)
# For self-hosted Langfuse, set to your server URL
# Example: LANGFUSE_HOST=http://your-langfuse-server:9019
# Note: See langfuse/README.md for self-hosting setup
LANGFUSE_HOST=

# ===== Testing Configuration =====
# Optional: Allow tests to make real network calls.
# Defaults to false; tests will skip network calls when not explicitly enabled.
TEST_WITH_NETWORK=false

# ===== Streamlit Server Configuration =====
# Optional: Streamlit server configuration (override defaults if needed)
# STREAMLIT_SERVER_PORT=8501
# STREAMLIT_SERVER_ADDRESS=0.0.0.0

